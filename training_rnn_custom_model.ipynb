{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LET'S DO THISS!!! :DDDD\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Lets check your GPU\n",
    "if tf.config.list_logical_devices('GPU') != []:\n",
    "    print(\"LET'S DO THISS!!! :DDDD\")\n",
    "else: \n",
    "    print(\"Oh dude... D:\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first citizen:\n",
      "before we proceed any further, hear me speak.\n",
      "\n",
      "all:\n",
      "speak, speak.\n",
      "\n",
      "first citizen:\n",
      "you\n"
     ]
    }
   ],
   "source": [
    "with open('data/input.txt') as f:\n",
    "    content = f.read().lower()\n",
    "    \n",
    "print(content[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n !$&',-.3:;?abcdefghijklmnopqrstuvwxyz\""
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\".join(sorted(set(content.lower())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_vec_layer = keras.layers.TextVectorization(\n",
    "                                                split=\"character\",\n",
    "                                                standardize=\"lower\"\n",
    "                                            )\n",
    "text_vec_layer.adapt([content])\n",
    "encoded = text_vec_layer([content])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded -= 2  # drop tokens 0 (pad) and 1 (unknown), subtraindo 2 (pad e unknown)\n",
    "n_tokens = text_vec_layer.vocabulary_size() - 2  # number of distinct chars = 39\n",
    "dataset_size = len(encoded)  # total number of chars = 1,115,394"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(39, 1115394)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tokens, dataset_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128\n",
    "batch_size = 32\n",
    "ds = tf.data.Dataset.from_tensor_slices(encoded) # Fatia o ds em janelas de tamanho window_size\n",
    "ds = ds.window(window_size + 1, shift=1, drop_remainder=True) # Você pode visualizar com um .as_numpy_iterator() e colocar num loop com .take(1)\n",
    "ds = ds.flat_map(lambda window: window.batch(window_size + 1)) # Transforma cada janela em um tensor\n",
    "ds = ds.shuffle(100_000, seed=42)\n",
    "ds = ds.batch(batch_size)\n",
    "ds = ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(32, 128) (32, 128)\n",
      "tf.Tensor(\n",
      "[ 9 12  0 19  5 20  6  2  0 16  5  2  6  0  2 27  3  2  6  1  8 17 10  1\n",
      "  8  1  0  7  2  4 15  0 21  1  6  5  9 12  0  2  6  5  7  0 21 13  7  5\n",
      "  9  1  7  7 26 10 10 14  1  9  1  9  5 13  7 23 10  3 17  0  2  8 13  1\n",
      " 31 21  8  1 12 30 10 10 19  5  8  7  2  0  7  1  9  4  2  3  8 23 10 15\n",
      "  3 13  8  0 18  3 14 22  4  9 15  0  2  3  0  2  6  1  0 18  4 22  5  2\n",
      "  3 11 28  0 16  6  1  8], shape=(128,), dtype=int64) tf.Tensor(\n",
      "[12  0 19  5 20  6  2  0 16  5  2  6  0  2 27  3  2  6  1  8 17 10  1  8\n",
      "  1  0  7  2  4 15  0 21  1  6  5  9 12  0  2  6  5  7  0 21 13  7  5  9\n",
      "  1  7  7 26 10 10 14  1  9  1  9  5 13  7 23 10  3 17  0  2  8 13  1 31\n",
      " 21  8  1 12 30 10 10 19  5  8  7  2  0  7  1  9  4  2  3  8 23 10 15  3\n",
      " 13  8  0 18  3 14 22  4  9 15  0  2  3  0  2  6  1  0 18  4 22  5  2  3\n",
      " 11 28  0 16  6  1  8  1], shape=(128,), dtype=int64)\n"
     ]
    }
   ],
   "source": [
    "for X_batch, y_batch in ds.take(1):\n",
    "    print(X_batch.shape, y_batch.shape)\n",
    "    print(X_batch[0], y_batch[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criando função\n",
    "def to_dataset(sequence, window_size=128, batch_size=32, seed=42, shuffle=False, target='all_window'):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(sequence) # Fatia o ds em janelas de tamanho window_size\n",
    "    ds = ds.window(window_size + 1, shift=1, drop_remainder=True) # Você pode visualizar com um .as_numpy_iterator() e colocar num loop com .take(1)\n",
    "    ds = ds.flat_map(lambda window: window.batch(window_size + 1)) # Transforma cada janela em um tensor\n",
    "    if shuffle:\n",
    "        ds = ds.shuffle(100_000, seed=seed)\n",
    "    ds = ds.batch(batch_size)\n",
    "    if target == 'all_window':\n",
    "        ds = ds.map(lambda window: (window[:, :-1], window[:, 1:])).prefetch(1)\n",
    "    elif target == 'last_char':\n",
    "        ds = ds.map(lambda window: (window[:, :-1], window[:, -1])).prefetch(1)\n",
    "    else:\n",
    "        raise ValueError('target must be \"all_window\" or \"last_char\"')\n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(<tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "  array([[ 4,  5,  2],\n",
       "         [ 5,  2, 23]], dtype=int64)>,\n",
       "  <tf.Tensor: shape=(2, 3), dtype=int64, numpy=\n",
       "  array([[ 5,  2, 23],\n",
       "         [ 2, 23,  3]], dtype=int64)>)]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Exemplo de uso\n",
    "list(to_dataset(text_vec_layer(['To be'])[0], window_size=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 128\n",
    "tf.random.set_seed(42)\n",
    "train_set = to_dataset(encoded[:1_000_000], window_size=window_size, shuffle=True, seed=42)\n",
    "valid_set = to_dataset(encoded[1_000_000:1_060_000], window_size=window_size)\n",
    "test_set = to_dataset(encoded[1_060_000:], window_size=window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Char-RNN model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomModel(keras.Model):\n",
    "    def __init__(self, n_tokens):\n",
    "        super(CustomModel, self).__init__()\n",
    "        self.embedding = keras.layers.Embedding(input_dim=n_tokens, \n",
    "                                                output_dim=16, \n",
    "                                                input_shape=[None, 128])\n",
    "        self.conv1d = keras.layers.Conv1D(32, kernel_size=3, padding=\"causal\", activation=\"relu\")\n",
    "        self.gru = keras.layers.GRU(128, return_sequences=True)\n",
    "        self.dense = keras.layers.Dense(n_tokens, activation=\"softmax\")\n",
    "        \n",
    "    def call(self, inputs, training=False):\n",
    "        x = self.embedding(inputs)\n",
    "        x = self.conv1d(x)\n",
    "        x = self.gru(x)\n",
    "        x = self.dense(x)\n",
    "        return x\n",
    "    \n",
    "    def train_step(self,data):\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:\n",
    "            y_pred = self(x, training=True) # Forward pass\n",
    "            loss = self.compiled_loss(y, y_pred, regularization_losses=self.losses)\n",
    "        gradients = tape.gradient(loss, self.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(gradients, self.trainable_variables))\n",
    "        self.compiled_metrics.update_state(y, y_pred)\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "    \n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "model = CustomModel(n_tokens=n_tokens)\n",
    "model.compile(\n",
    "    loss=\"sparse_categorical_crossentropy\", \n",
    "    optimizer=\"nadam\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X_batch, y_batch in train_set.take(1):\n",
    "    print(X_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model(X_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"custom_model_11\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_10 (Embedding)    multiple                  624       \n",
      "                                                                 \n",
      " conv1d_10 (Conv1D)          multiple                  1568      \n",
      "                                                                 \n",
      " gru_10 (GRU)                multiple                  62208     \n",
      "                                                                 \n",
      " dense_10 (Dense)            multiple                  5031      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 69,431\n",
      "Trainable params: 69,431\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "# estilo\n",
    "plt.style.use('dark_background')\n",
    "plt.rcParams['axes.facecolor'] = '#26262e' \n",
    "\n",
    "def print_multiple_generated(predicted_text, original_text, n=8):\n",
    "    print(\"--> Original text:\")\n",
    "    for i in range(n):\n",
    "        print(original_text[i])\n",
    "    print(\"\\n--> Generated text:\")\n",
    "    for i in range(n):\n",
    "        print(predicted_text[i])\n",
    "        \n",
    "def plot_loss(history, step):\n",
    "    plt.plot(list(range(len(history['loss']))), history['loss'], label='Training Loss')\n",
    "    plt.plot(list(range(len(history['loss']))), history['val_loss'], label='Validation Loss')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def plot_accuracy(history, step):\n",
    "    plt.plot(list(range(len(history['accuracy']))), history['accuracy'], label='Training Accuracy')\n",
    "    plt.plot(list(range(len(history['accuracy']))), history['val_accuracy'], label='Validation Accuracy')\n",
    "    plt.xlabel('Step')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = {\n",
    "            'text_base': 'To be or not ',\n",
    "            'text_base_encoded': text_vec_layer(['To be or not ']),\n",
    "            'list_predicted_text_base': [],\n",
    "        }\n",
    "\n",
    "class LogCallback(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, history):\n",
    "        super().__init__()\n",
    "        self.history_ = history\n",
    "    \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        print(f\"Final da época {epoch}, Loss: {logs['loss']}, Acc: {logs['accuracy']}\")\n",
    "    \n",
    "    def on_train_batch_end(self, batch, logs=None):\n",
    "        if batch % 1_000 == 0:\n",
    "            print(\"\")\n",
    "            print(f\"Final do lote {batch}, Loss do lote: {logs['loss']}\")\n",
    "            model = self.model\n",
    "            # predicted_text = decoder(tf.argmax(model(history['text_base_encoded'], training=False), axis=2))[0]\n",
    "            predicted_text = extend_text(\"To be or not \", temperature=1.0)\n",
    "            print(\"\")\n",
    "            print(\"--> Original text: \", history['text_base'])\n",
    "            print(\"--> Generated text: \", predicted_text)\n",
    "            self.history_['list_predicted_text_base'].append(predicted_text)\n",
    "            print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ckpt = tf.keras.callbacks.ModelCheckpoint(\n",
    "    \"my_shakespeare_model\", monitor=\"val_accuracy\", save_best_only=True)\n",
    "\n",
    "shakespeare_model = tf.keras.Sequential([\n",
    "    text_vec_layer,\n",
    "    tf.keras.layers.Lambda(lambda X: X - 2),  # no <PAD> or <UNK> tokens\n",
    "    model\n",
    "])\n",
    "\n",
    "tf.random.set_seed(42)  # extra code – ensures reproducibility on CPU\n",
    "\n",
    "def next_char(text, temperature=1):\n",
    "    y_proba = shakespeare_model.predict([text], verbose=0)[0, -1:]\n",
    "    rescaled_logits = tf.math.log(y_proba) / temperature\n",
    "    char_id = tf.random.categorical(rescaled_logits, num_samples=1)[0, 0]\n",
    "    return text_vec_layer.get_vocabulary()[char_id + 2]\n",
    "\n",
    "def extend_text(text, n_chars=50, temperature=1):\n",
    "    for _ in range(n_chars):\n",
    "        text += next_char(text, temperature)\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history_train = model.fit(train_set, \n",
    "                    validation_data=valid_set, \n",
    "                    epochs=10,\n",
    "                    callbacks=[LogCallback(history=history), \n",
    "                               model_ckpt])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "toydl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
